<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Publications, Projects, and Technical Reports of Debangshu Banerjee">
    <title>Publications & Projects - Debangshu Banerjee</title>
    <link rel="stylesheet" href="style.css">
    <script>
        function toggleAbstract(abstractId) {
            var abstract = document.getElementById(abstractId);
            if (abstract.style.display === "none") {
                abstract.style.display = "block";
            } else {
                abstract.style.display = "none";
            }
        }
    </script>
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="publications.html">Publications & Projects</a></li>
            </ul>
        </nav>
    </header>

    <section class="publications-section">
        <h2>Preprints</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "Towards Reliable Alignment: Uncertainty Aware RLHF", 2024. (Under Review)</p>
            <div><a href="javascript:void(0)" onclick="toggleAbstract('abstract1')" class="publication-link">Read Abstract</a></div>
            <p id="abstract1" class="abstract" style="display:none;">Abstract: Recent advances in aligning Large Language Models with human preferences have benefited from larger reward models and better preference data. However, most of these methodologies rely on the accuracy of the reward model... [trimmed for brevity]</p>
            <div><a href = "https://arxiv.org/pdf/2410.23726" target="_blank" class="publication-link">Read Report</a></div>        
        </div>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "Bad Values but Good Behavior: Learning Highly Misspecified Bandits and MDPs", 2023. (Under review)</p>
            <div><a href="javascript:void(0)" onclick="toggleAbstract('abstract2')" class="publication-link">Read Abstract</a></div>
            <p id="abstract2" class="abstract" style="display:none;">Abstract: Parametric, feature-based reward models are employed by a variety of algorithms in decision-making settings such as bandits and Markov decision processes (MDPs)... [trimmed for brevity]</p>
            <div><a href="https://arxiv.org/pdf/2310.09358" target="_blank" class="publication-link">Read Report</a></div>
        </div>
    </section>

    <section class="publications-section">
        <h2>Publications</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee, A. Ghosh, S. Ray Chowdhury, and A. Gopalan</strong>, "Exploration in linear bandits with rich action sets and its implications for inference", <em>Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</em>, PMLR, 2023.</p>
            <div><a href="javascript:void(0)" onclick="toggleAbstract('abstract3')" class="publication-link">Read Abstract</a></div>
            <p id="abstract3" class="abstract" style="display:none;">Abstract: We present a non-asymptotic lower bound on the spectrum of the design matrix generated by any linear bandit algorithm with sub-linear regret when the action set has well-behaved curvature... [trimmed for brevity]</p>
            <div><a href="https://proceedings.mlr.press/v206/banerjee23b/banerjee23b.pdf" target="_blank" class="publication-link">Read Paper</a></div>
            <div class="video-container">
                <iframe width="560" height="315" src="https://www.youtube.com/embed/CNeFh4l48sw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>
    </section>

    <section class="publications-section">
        <h2>Projects</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "On the minimax regret for linear bandits in a wide variety of action spaces", arXiv preprint, 2023.</p>
            <div><a href="javascript:void(0)" onclick="toggleAbstract('abstract4')" class="publication-link">Read Abstract</a></div>
            <p id="abstract4" class="abstract" style="display:none;">Abstract: As noted in the works of Lattimore and SzepesvÂ´ari [2020], it has been mentioned that it is an open problem to characterize the minimax regret of linear bandits in a wide variety of action spaces...</p>
            <div><a href="https://arxiv.org/abs/2301.03597" target="_blank" class="publication-link">Read Report</a></div>
        </div>
        <div class="publication-item">
            <p><strong>Deep Reinforcement Learning based on Human Feedback</strong> (2023) - Constructed demos for the tutorial "Do you prefer learning from preferences" at NeurIPS 2023.</p>
        </div>
        <div class="publication-item">
            <p><strong>How Reliable are Test Numbers for Revealing the COVID-19 Ground Truth?</strong> (2021) - Data visualization project for understanding COVID-19 numbers.</p>
            <div><a href="https://arxiv.org/pdf/2004.12782.pdf" target="_blank" class="publication-link">Read Paper</a></div>
        </div>
        <div class="publication-item">
            <p><strong>D. Banerjee</strong>, "Hex and Neurodynamic Programming", arXiv preprint, 2017.</p>
            <div><a href="javascript:void(0)" onclick="toggleAbstract('abstract5')" class="publication-link">Read Abstract</a></div>
            <p id="abstract5" class="abstract" style="display:none;">Abstract: Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning... [trimmed for brevity]</p>
            <div><a href="https://arxiv.org/pdf/2008.06359.pdf" target="_blank" class="publication-link">Read Report</a></div>
        </div>
    </section>

    <footer>
        <p>&copy; 2024 Debangshu Banerjee</p>
    </footer>
</body>
</html>
