<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Publications, Projects, and Technical Reports of Debangshu Banerjee">
    <title>Publications & Projects - Debangshu Banerjee</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="about.html">About Me</a></li>
                <li><a href="publications.html">Publications & Projects</a></li>
            </ul>
        </nav>
    </header>

    <section class="publications-section">
        <h2>Preprints</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "Towards Reliable Alignment: Uncertainty Aware RLHF", 2024. (Under Review)</p>
            <p class="abstract">Abstract: Recent advances in aligning Large Language Models with human preferences have benefited from larger reward models and better preference data. However, most of these methodologies rely on the accuracy of the reward model. The reward models used in Reinforcement Learning with Human Feedback (RLHF) are typically learned from small datasets using stochastic optimization algorithms, making them prone to high variability. We illustrate the inconsistencies between reward models empirically on numerous open-source datasets.We theoretically show that the fluctuation of the reward models can be detrimental to the alignment problem because the derived policies are more overfitted to the reward model and, hence, are riskier if the reward model itself is uncertain. We use concentration of measure to motivate an uncertainty-aware, conservative algorithm for policy optimization. We show that such policies are more risk-averse in the sense that they are more cautious of uncertain rewards. We theoretically prove that our proposed methodology has less risk than the vanilla method. We corroborate our theoretical results with experiments based on designing an ensemble of reward models. We use this ensemble of reward models to align a language model using our methodology and observe that our empirical findings match our theoretical predictions.</p>
        </div>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "Bad Values but Good Behavior: Learning Highly Misspecified Bandits and MDPs", 2023. (Under review)</p>
            <p class="abstract">Abstract: Parametric, feature-based reward models are employed by a variety of algorithms in decision-making settings such as bandits and Markov decision processes (MDPs). The typical assumption under which the algorithms are analysed is realizability, i.e., that the true values of actions are perfectly explained by some parametric model in the class. We are, however, interested in the situation where the true values are (significantly) misspecified with respect to the model class. For parameterized bandits, contextual bandits and MDPs, we identify structural conditions, depending on the problem instance and model class, under which basic algorithms such as $\epsilon$-greedy, LinUCB and fitted Q-learning provably learn optimal policies under even highly misspecified models. This is in contrast to existing worst-case results for, say misspecified bandits, which show regret bounds that incur a linear scaling with time horizon, and shows that there can be a nontrivially large set of bandit instances that are robust to misspecification.</p>
            <a href="https://arxiv.org/pdf/2310.09358" target="_blank" class="publication-link">Read Report</a>
        </div>
    </section>

    <section class="publications-section">
        <h2>Publications</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee, A. Ghosh, S. Ray Chowdhury, and A. Gopalan</strong>, "Exploration in linear bandits with rich action sets and its implications for inference", <em>Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</em>, PMLR, 2023.</p>
            <p class="abstract">Abstract: We present a non-asymptotic lower bound on the spectrum of the design matrix generated by any linear bandit algorithm with sub-linear regret when the action set has well-behaved curvature. Specifically, we show that the minimum eigenvalue of the expected design matrix grows as Ω(√n) whenever the expected cumulative regret of the algorithm is O(√n), where n is the learning horizon, and the action-space has a constant Hessian around the optimal arm. This shows that such action-spaces force a polynomial lower bound on the least eigenvalue, rather than a logarithmic lower bound as shown by Lattimore and Szepesvari (2017) for discrete (i.e., well-separated) action spaces. Furthermore, while the latter holds only in the asymptotic regime (n → ∞), our result for these “locally rich” action spaces is any-time. Additionally, under a mild technical assumption, we obtain a similar lower bound on the minimum eigen value holding with high probability. We apply our result to two practical scenarios – model selection and clustering in linear bandits. For model selection, we show that an epoch-based linear bandit al- gorithm adapts to the true model complexity at a rate exponential in the number of epochs, by virtue of our novel spectral bound. For cluster- ing, we consider a multi agent framework where we show, by leveraging the spectral result, that no forced exploration is necessary—the agents can run a linear bandit algorithm and estimate their underlying parameters at once, and hence incur a low regret.</p>
            <a href="https://proceedings.mlr.press/v206/banerjee23b/banerjee23b.pdf" target="_blank" class="publication-link">Read Paper</a>
        </div>
    </section>

    <section class="publications-section">
        <h2>Projects</h2>
        <div class="publication-item">
            <p><strong>D. Banerjee and A. Gopalan</strong>, "On the minimax regret for linear bandits in a wide variety of action spaces", arXiv preprint, 2023.</p>
            <p class="abstract">Abstract: As noted in the works of Lattimore and Szepesv´ari [2020], it has been mentioned that it is an open problem to characterize the minimax regret of linear bandits in a wide variety of action spaces. In this article we present an optimal regret lower bound for a wide class of convex action spaces.</p>
            <a href="https://arxiv.org/abs/2301.03597" target="_blank" class="publication-link">Read Report</a>
        </div>
        <div class="publication-item">
            <p><strong>Deep Reinforcement Learning based on Human Feedback</strong> (2023) - Constructed demos for the tutorial "Do you prefer learning from preferences" at NeurIPS 2023.</p>
        </div>
        <div class="publication-item">
            <p><strong>How Reliable are Test Numbers for Revealing the COVID-19 Ground Truth?</strong> (2021) - Data visualization project for understanding COVID-19 numbers.</p>
            <a href="https://arxiv.org/pdf/2004.12782.pdf" target="_blank" class="publication-link">Read Paper</a>
        </div>
        <div class="publication-item">
            <p><strong>D. Banerjee</strong>, "Hex and Neurodynamic Programming", arXiv preprint, 2017.</p>
            <p class="abstract">Abstract: Hex is a complex game with a high branching factor. For the first time Hex is being attempted to be solved without the use of game tree structures and associated methods of pruning. We also are abstaining from any heuristic information about Virtual Connections or Semi Virtual Connections which were previously used in all previous known computer versions of the game. The H-search algorithm which was the basis of finding such connections and had been used with success in previous Hex playing agents has been forgone. Instead what we use is reinforcement learning through self play and approximations through neural networks to by pass the problem of high branching factor and maintaining large tables for state-action evaluations. Our code is based primarily on NeuroHex. The inspiration is drawn from the recent success of AlphaGo Zero.</p>
            <a href="https://arxiv.org/pdf/2008.06359.pdf" target="_blank" class="publication-link">Read Report</a>
        </div>
    </section>

    <footer>
        <p>&copy; 2024 Debangshu Banerjee</p>
    </footer>
</body>
</html>
